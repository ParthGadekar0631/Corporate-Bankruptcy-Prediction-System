{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "yyCqH_qS-Jrt"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMxCvhHE-P6r"
      },
      "source": [
        "### =================\n",
        "### Load test dataset\n",
        "### ================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "1tfnG23H-OKn"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Loading test dataset...\n",
            "Test rows: 1012\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nLoading test dataset...\")\n",
        "test_df = pd.read_csv(\"test_data.csv\")\n",
        "print(\"Test rows:\", len(test_df))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIK7Jtlp-S_8"
      },
      "source": [
        "### =============================\n",
        "### LOAD CLUSTER ASSIGNMENT MODEL\n",
        "### ============================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "rhFEKYdq-W9A"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Assigning clusters to test data...\n",
            "\n",
            "Cluster distribution:\n",
            "Assigned_Cluster\n",
            "2    371\n",
            "3    354\n",
            "0    248\n",
            "1     39\n",
            "Name: count, dtype: int64 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "cluster_joblib = joblib.load(\"models/cluster_id_model.joblib\")\n",
        "cluster_model   = cluster_joblib[\"model\"]           # Pipeline (scaler + RF)\n",
        "cluster_feats   = cluster_joblib[\"feature_cols\"]    # 95 Stage-1 features\n",
        "\n",
        "\n",
        "print(\"\\nAssigning clusters to test data...\")\n",
        "X_cluster = test_df.drop(columns=[\"Index\"], errors=\"ignore\")[cluster_feats]\n",
        "scaled_in = cluster_model.named_steps[\"scaler\"].transform(X_cluster)\n",
        "\n",
        "test_df[\"Assigned_Cluster\"] = cluster_model.named_steps[\"rf\"].predict(scaled_in)\n",
        "\n",
        "print(\"\\nCluster distribution:\")\n",
        "print(test_df[\"Assigned_Cluster\"].value_counts(), \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vaqYUEIF-iDA"
      },
      "source": [
        "### ==============================\n",
        "### Load Available Subgroup Models\n",
        "### =============================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dummy reconstruction to allow unpickling\n",
        "class ConstantModel:\n",
        "    def predict(self, X):\n",
        "        import numpy as np\n",
        "        return np.zeros(len(X))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✔ Loaded model for cluster 0\n",
            "✔ Loaded model for cluster 2\n",
            "✔ Loaded model for cluster 3\n",
            "✔ Loaded model for cluster 5\n",
            "\n",
            "Active Models: [0, 2, 3, 5]\n"
          ]
        }
      ],
      "source": [
        "subgroup_models = {}\n",
        "\n",
        "for f in os.listdir(\"models\"):\n",
        "    if f.startswith(\"subgroup\") and f.endswith(\"_model.joblib\"):\n",
        "        cid = int(f.split(\"_cluster\")[1].split(\"_\")[0])\n",
        "        try:\n",
        "            subgroup_models[cid] = joblib.load(f\"models/{f}\")\n",
        "            print(f\"✔ Loaded model for cluster {cid}\")\n",
        "        except Exception as e:\n",
        "            print(f\"⚠ Could not load {f} → {e}\")\n",
        "\n",
        "print(\"\\nActive Models:\", list(subgroup_models.keys()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbS4hx0L-oQK"
      },
      "source": [
        "### ============================================\n",
        "### Predict Bankruptcy for Each Assigned Cluster\n",
        "### ============================================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "ko4ugWiA-mXc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================= FINAL CLUSTER PREDICTION REPORT =================\n",
            "\n",
            "\n",
            "============== CLUSTER 0 ==============\n",
            "Total rows in this cluster = 248\n",
            "Cluster 0 → Using Parth subgroup stacking model\n",
            "Cluster 0 → bankrupt predicted = 42\n",
            "\n",
            "============== CLUSTER 1 ==============\n",
            "Total rows in this cluster = 39\n",
            "⚠ No model found for cluster 1 → marking bankrupt = 0 for all\n",
            "\n",
            "============== CLUSTER 2 ==============\n",
            "Total rows in this cluster = 371\n",
            "Cluster 2 → Using PCA+Oversampling tuned model (with threshold)\n",
            "Cluster 2 → bankrupt = 2 (thr=0.01)\n",
            "\n",
            "============== CLUSTER 3 ==============\n",
            "Total rows in this cluster = 354\n",
            "Cluster 3 → Using saved stacking model with stored best threshold\n",
            "Cluster 3 → bankrupt predicted = 5 (thr=0.9)\n",
            "\n",
            "============== CLUSTER 4 ==============\n",
            "Total rows in this cluster = 0\n",
            "⚠ No model found for cluster 4 → marking bankrupt = 0 for all\n",
            "\n",
            "============== CLUSTER 5 ==============\n",
            "Total rows in this cluster = 0\n",
            "Cluster 5 → ConstantModel detected (no predict_proba)\n",
            "Cluster 5 → bankrupt predicted = 0\n"
          ]
        }
      ],
      "source": [
        "pred = np.zeros(len(test_df), dtype=int)\n",
        "prob = np.zeros(len(test_df), dtype=float)\n",
        "print(\"\\n================= FINAL CLUSTER PREDICTION REPORT =================\\n\")\n",
        "\n",
        "for cid in range(6):   # handles clusters 0–5\n",
        "    \n",
        "    idx = test_df.index[test_df[\"Assigned_Cluster\"] == cid]\n",
        "\n",
        "    print(f\"\\n============== CLUSTER {cid} ==============\")\n",
        "    print(f\"Total rows in this cluster = {len(idx)}\")\n",
        "\n",
        "    # CASE 1 → No subgroup model exists (clusters 1 & 4 OR missing)\n",
        "    if cid not in subgroup_models:\n",
        "        print(f\"⚠ No model found for cluster {cid} → marking bankrupt = 0 for all\")\n",
        "        pred[idx] = 0\n",
        "        prob[idx] = 0\n",
        "        continue\n",
        "\n",
        "    bundle = subgroup_models[cid]\n",
        "\n",
        "    # ---------------- CLUSTER 0 (Parth) ----------------\n",
        "    if cid == 0 and \"model\" in bundle:\n",
        "        print(\"Cluster 0 → Using Parth subgroup stacking model\")\n",
        "        model = bundle[\"model\"]\n",
        "        feats = bundle[\"selected_features\"]\n",
        "        X_sub = test_df.iloc[idx][feats]\n",
        "        sub_prob = model.predict_proba(X_sub)[:, 1]\n",
        "        prob[idx] = sub_prob\n",
        "        pred[idx] = (sub_prob >= 0.55).astype(int)\n",
        "        print(f\"Cluster 0 → bankrupt predicted = {pred[idx].sum()}\")\n",
        "\n",
        "    # ---------------- CLUSTER 2 (Dhanisha) ----------------\n",
        "    elif cid == 2 and \"pipeline\" in bundle:\n",
        "        print(\"Cluster 2 → Using PCA+Oversampling tuned model (with threshold)\")\n",
        "        model = bundle[\"pipeline\"]\n",
        "        feats = bundle[\"feature_names\"]\n",
        "        thr   = bundle[\"threshold\"]\n",
        "        X_sub = test_df.iloc[idx][feats]\n",
        "        sub_prob = model.predict_proba(X_sub)[:, 1]\n",
        "        prob[idx] = sub_prob\n",
        "        pred[idx] = (sub_prob >= thr).astype(int)\n",
        "        print(f\"Cluster 2 → bankrupt = {pred[idx].sum()} (thr={thr})\")\n",
        "\n",
        "    # ---------------- CLUSTER 3 (Lasya) ----------------\n",
        "    elif cid == 3:\n",
        "        print(\"Cluster 3 → Using saved stacking model with stored best threshold\")\n",
        "\n",
        "        # Load saved object from joblib (pipeline + best_threshold)\n",
        "        obj = joblib.load(\"models/subgroup3_cluster3_model.joblib\")\n",
        "        pipeline = obj[\"pipeline\"]\n",
        "        best_t   = obj[\"best_threshold\"]\n",
        "\n",
        "       # Ensure 'cluster_id' column exists because pipeline.feature_names_in_ expects it\n",
        "        temp = test_df.copy()\n",
        "        if \"cluster_id\" not in temp.columns:\n",
        "            temp[\"cluster_id\"] = temp[\"Assigned_Cluster\"]\n",
        "\n",
        "            # Extract the exact features used during training\n",
        "            feat_3 = list(pipeline.feature_names_in_)\n",
        "\n",
        "        # Select matching features\n",
        "        X_sub = temp.iloc[idx][feat_3]\n",
        "\n",
        "        # Predict\n",
        "        sub_prob = pipeline.predict_proba(X_sub)[:, 1]\n",
        "        prob[idx] = sub_prob\n",
        "        pred[idx] = (sub_prob >= best_t).astype(int)\n",
        "\n",
        "        print(f\"Cluster 3 → bankrupt predicted = {pred[idx].sum()} (thr={best_t})\")\n",
        "\n",
        "    # ---------------- CLUSTER 5 (Premlata – Constant Model) ----------------\n",
        "    elif cid == 5:\n",
        "        print(\"Cluster 5 → ConstantModel detected (no predict_proba)\")\n",
        "\n",
        "        X_sub = test_df.iloc[idx].drop(columns=[\"Bankrupt?\"], errors=\"ignore\")\n",
        "\n",
        "        sub_pred = cm_model.predict(X_sub)       # always 1\n",
        "        pred[idx] = sub_pred\n",
        "        prob[idx] = np.ones(len(idx)) * 0.99     # assign high confidence manually\n",
        "\n",
        "        print(f\"Cluster 5 → bankrupt predicted = {pred[idx].sum()}\")\n",
        "\n",
        "\n",
        "    # ---------------- Unexpected case ----------------\n",
        "    else:\n",
        "        print(\"Unexpected model format — marking all 0\")\n",
        "        pred[idx] = 0\n",
        "        prob[idx] = 0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ============================\n",
        "### Export Final Submission File\n",
        "### ============================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==============================================================\n",
            "     FINAL SUBMISSION SAVED → Group4_Generalization.csv\n",
            "==============================================================\n",
            "   Index  Bankrupt?\n",
            "0      0          0\n",
            "1      1          0\n",
            "2      2          0\n",
            "3      3          0\n",
            "4      4          0\n"
          ]
        }
      ],
      "source": [
        "test_df[\"Bankrupt?\"] = pred.astype(int)\n",
        "\n",
        "out = test_df[[\"Index\",\"Bankrupt?\"]]\n",
        "out.to_csv(\"Group4_Generalization.csv\", index=False)\n",
        "\n",
        "print(\"\\n==============================================================\")\n",
        "print(\"     FINAL SUBMISSION SAVED → Group4_Generalization.csv\")\n",
        "print(\"==============================================================\")\n",
        "print(out.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total predicted bankrupt: 49\n",
            "Percentage bankrupt: 4.841897233201581 %\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_gen = pd.read_csv(\"Group4_Generalization.csv\")\n",
        "\n",
        "total_bankrupt = df_gen[\"Bankrupt?\"].sum()\n",
        "percent_bankrupt = (df_gen[\"Bankrupt?\"].mean()) * 100\n",
        "\n",
        "print(\"Total predicted bankrupt:\", total_bankrupt)\n",
        "print(\"Percentage bankrupt:\", percent_bankrupt, \"%\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
